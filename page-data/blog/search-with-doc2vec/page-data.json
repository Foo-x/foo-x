{"componentChunkName":"component---src-templates-blog-post-js","path":"/blog/search-with-doc2vec/","result":{"data":{"markdownRemark":{"fields":{"slug":"/search-with-doc2vec/"},"id":"6c5b5aed-ddee-5c03-8d1d-f2703bbdd601","excerpt":"Doc2Vecで類似文章を検索してみたので、実装を紹介します。 Doc2Vecとは コンピュータが自然言語を処理するためには、まず人間の言葉をコンピュータで扱える値にする必要があります。 単語の意味をベクトル化する手法としてWord2Vec…","html":"<p>Doc2Vecで類似文章を検索してみたので、実装を紹介します。</p>\n<h2 id=\"doc2vecとは\" style=\"position:relative;\"><a href=\"#doc2vec%E3%81%A8%E3%81%AF\" aria-label=\"doc2vecとは permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Doc2Vecとは</h2>\n<p>コンピュータが自然言語を処理するためには、まず人間の言葉をコンピュータで扱える値にする必要があります。<br>\n単語の意味をベクトル化する手法として<a href=\"https://deepage.net/bigdata/machine_learning/2016/09/02/word2vec_power_of_word_vector.html\">Word2Vec</a>が存在します。<br>\n詳しくはリンク先がとてもわかりやすいのですが、ざっくり言うと前後n単語のリストでその単語を表現します。<br>\nこうすることで、例えば「犬」と「猫」は同じような文脈で使われるため、似た「意味」であると考えることができます。<br>\nDoc2VecはWord2Vecを応用し、文章をベクトル化するものです。</p>\n<h2 id=\"実装サンプル\" style=\"position:relative;\"><a href=\"#%E5%AE%9F%E8%A3%85%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB\" aria-label=\"実装サンプル permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>実装サンプル</h2>\n<p>今回Doc2Vecを用いて実現するのは、以下の2つの機能です。</p>\n<ul>\n<li>単語で文章を検索</li>\n<li>類似文章の検索</li>\n</ul>\n<p>サンプルとして、青空文庫の文章を使用しました。<br>\nなお、この記事で使用するコードは<a href=\"https://github.com/Foo-x/doc2vec-sample\">GitHubで公開しています</a>。<br>\n(学習に使用した文章もzipにしましたが、サイズが大きいので注意してください)</p>\n<h3 id=\"環境\" style=\"position:relative;\"><a href=\"#%E7%92%B0%E5%A2%83\" aria-label=\"環境 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>環境</h3>\n<ul>\n<li>Python 3</li>\n<li>MeCab</li>\n<li>gensim</li>\n</ul>\n<p>を使えるようにしてください。</p>\n<h3 id=\"学習\" style=\"position:relative;\"><a href=\"#%E5%AD%A6%E7%BF%92\" aria-label=\"学習 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>学習</h3>\n<ol>\n<li>文章のファイルを取得</li>\n<li>ファイルから文章を取得</li>\n<li>文章から不要な部分を削除</li>\n<li>単語に分解</li>\n<li>Doc2Vecで学習</li>\n<li>学習データを出力</li>\n</ol>\n<p>の流れで処理します。</p>\n<h4 id=\"1-文章のファイルを取得\" style=\"position:relative;\"><a href=\"#1-%E6%96%87%E7%AB%A0%E3%81%AE%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%E3%82%92%E5%8F%96%E5%BE%97\" aria-label=\"1 文章のファイルを取得 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. 文章のファイルを取得</h4>\n<div class=\"gatsby-highlight\" data-language=\"py\"><pre class=\"language-py\"><code class=\"language-py\"><span class=\"token keyword\">import</span> os\n<span class=\"token keyword\">import</span> sys\n<span class=\"token keyword\">import</span> MeCab\n<span class=\"token keyword\">import</span> collections\n<span class=\"token keyword\">from</span> gensim <span class=\"token keyword\">import</span> models\n<span class=\"token keyword\">from</span> gensim<span class=\"token punctuation\">.</span>models<span class=\"token punctuation\">.</span>doc2vec <span class=\"token keyword\">import</span> LabeledSentence</code></pre></div>\n<p>まずは必要ライブラリのインポートです。</p>\n<div class=\"gatsby-highlight\" data-language=\"py\"><pre class=\"language-py\"><code class=\"language-py\"><span class=\"token keyword\">def</span> <span class=\"token function\">get_all_files</span><span class=\"token punctuation\">(</span>directory<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">for</span> root<span class=\"token punctuation\">,</span> dirs<span class=\"token punctuation\">,</span> files <span class=\"token keyword\">in</span> os<span class=\"token punctuation\">.</span>walk<span class=\"token punctuation\">(</span>directory<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">for</span> <span class=\"token builtin\">file</span> <span class=\"token keyword\">in</span> files<span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">yield</span> os<span class=\"token punctuation\">.</span>path<span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span>root<span class=\"token punctuation\">,</span> <span class=\"token builtin\">file</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>与えられたディレクトリ以下のファイルをすべて取得します。</p>\n<h4 id=\"2-ファイルから文章を取得\" style=\"position:relative;\"><a href=\"#2-%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%E3%81%8B%E3%82%89%E6%96%87%E7%AB%A0%E3%82%92%E5%8F%96%E5%BE%97\" aria-label=\"2 ファイルから文章を取得 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. ファイルから文章を取得</h4>\n<div class=\"gatsby-highlight\" data-language=\"py\"><pre class=\"language-py\"><code class=\"language-py\"><span class=\"token keyword\">def</span> <span class=\"token function\">read_document</span><span class=\"token punctuation\">(</span>path<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">with</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span>path<span class=\"token punctuation\">,</span> <span class=\"token string\">'r'</span><span class=\"token punctuation\">,</span> encoding<span class=\"token operator\">=</span><span class=\"token string\">'sjis'</span><span class=\"token punctuation\">,</span> errors<span class=\"token operator\">=</span><span class=\"token string\">'ignore'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> f<span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> f<span class=\"token punctuation\">.</span>read<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h4 id=\"3-文章から不要な部分を削除\" style=\"position:relative;\"><a href=\"#3-%E6%96%87%E7%AB%A0%E3%81%8B%E3%82%89%E4%B8%8D%E8%A6%81%E3%81%AA%E9%83%A8%E5%88%86%E3%82%92%E5%89%8A%E9%99%A4\" aria-label=\"3 文章から不要な部分を削除 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. 文章から不要な部分を削除</h4>\n<div class=\"gatsby-highlight\" data-language=\"py\"><pre class=\"language-py\"><code class=\"language-py\"><span class=\"token keyword\">def</span> <span class=\"token function\">trim_doc</span><span class=\"token punctuation\">(</span>doc<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    lines <span class=\"token operator\">=</span> doc<span class=\"token punctuation\">.</span>splitlines<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    valid_lines <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n    is_valid <span class=\"token operator\">=</span> <span class=\"token boolean\">False</span>\n    horizontal_rule_cnt <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n    break_cnt <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n    <span class=\"token keyword\">for</span> line <span class=\"token keyword\">in</span> lines<span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">if</span> horizontal_rule_cnt <span class=\"token operator\">&lt;</span> <span class=\"token number\">2</span> <span class=\"token keyword\">and</span> <span class=\"token string\">'-----'</span> <span class=\"token keyword\">in</span> line<span class=\"token punctuation\">:</span>\n            horizontal_rule_cnt <span class=\"token operator\">+=</span> <span class=\"token number\">1</span>\n            is_valid <span class=\"token operator\">=</span> horizontal_rule_cnt <span class=\"token operator\">==</span> <span class=\"token number\">2</span>\n            <span class=\"token keyword\">continue</span>\n        <span class=\"token keyword\">if</span> <span class=\"token keyword\">not</span><span class=\"token punctuation\">(</span>is_valid<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">continue</span>\n        <span class=\"token keyword\">if</span> line <span class=\"token operator\">==</span> <span class=\"token string\">''</span><span class=\"token punctuation\">:</span>\n            break_cnt <span class=\"token operator\">+=</span> <span class=\"token number\">1</span>\n            is_valid <span class=\"token operator\">=</span> break_cnt <span class=\"token operator\">!=</span> <span class=\"token number\">3</span>\n            <span class=\"token keyword\">continue</span>\n        break_cnt <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n        valid_lines<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>line<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> <span class=\"token string\">''</span><span class=\"token punctuation\">.</span>join<span class=\"token punctuation\">(</span>valid_lines<span class=\"token punctuation\">)</span></code></pre></div>\n<p>ここは対象の文章によって処理が変わると思います。<br>\n今回は本文の前後にある文章の説明部分を無視するようにしました。<br>\nそもそもこれがどこまで精度に関わるのかは不明です。</p>\n<h4 id=\"4-単語に分解\" style=\"position:relative;\"><a href=\"#4-%E5%8D%98%E8%AA%9E%E3%81%AB%E5%88%86%E8%A7%A3\" aria-label=\"4 単語に分解 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>4. 単語に分解</h4>\n<div class=\"gatsby-highlight\" data-language=\"py\"><pre class=\"language-py\"><code class=\"language-py\"><span class=\"token keyword\">def</span> <span class=\"token function\">split_into_words</span><span class=\"token punctuation\">(</span>doc<span class=\"token punctuation\">,</span> name<span class=\"token operator\">=</span><span class=\"token string\">''</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    mecab <span class=\"token operator\">=</span> MeCab<span class=\"token punctuation\">.</span>Tagger<span class=\"token punctuation\">(</span><span class=\"token string\">\"-Ochasen\"</span><span class=\"token punctuation\">)</span>\n    valid_doc <span class=\"token operator\">=</span> trim_doc<span class=\"token punctuation\">(</span>doc<span class=\"token punctuation\">)</span>\n    lines <span class=\"token operator\">=</span> mecab<span class=\"token punctuation\">.</span>parse<span class=\"token punctuation\">(</span>doc<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>splitlines<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    words <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">for</span> line <span class=\"token keyword\">in</span> lines<span class=\"token punctuation\">:</span>\n        chunks <span class=\"token operator\">=</span> line<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token string\">'\\t'</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">if</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>chunks<span class=\"token punctuation\">)</span> <span class=\"token operator\">></span> <span class=\"token number\">3</span> <span class=\"token keyword\">and</span> <span class=\"token punctuation\">(</span>chunks<span class=\"token punctuation\">[</span><span class=\"token number\">3</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>startswith<span class=\"token punctuation\">(</span><span class=\"token string\">'動詞'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">or</span> chunks<span class=\"token punctuation\">[</span><span class=\"token number\">3</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>startswith<span class=\"token punctuation\">(</span><span class=\"token string\">'形容詞'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">or</span> <span class=\"token punctuation\">(</span>chunks<span class=\"token punctuation\">[</span><span class=\"token number\">3</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>startswith<span class=\"token punctuation\">(</span><span class=\"token string\">'名詞'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">and</span> <span class=\"token keyword\">not</span> chunks<span class=\"token punctuation\">[</span><span class=\"token number\">3</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>startswith<span class=\"token punctuation\">(</span><span class=\"token string\">'名詞-数'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            words<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>chunks<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> LabeledSentence<span class=\"token punctuation\">(</span>words<span class=\"token operator\">=</span>words<span class=\"token punctuation\">,</span> tags<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span>name<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">corpus_to_sentences</span><span class=\"token punctuation\">(</span>corpus<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    docs <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>read_document<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> corpus<span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">for</span> idx<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>doc<span class=\"token punctuation\">,</span> name<span class=\"token punctuation\">)</span> <span class=\"token keyword\">in</span> <span class=\"token builtin\">enumerate</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">zip</span><span class=\"token punctuation\">(</span>docs<span class=\"token punctuation\">,</span> corpus<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        sys<span class=\"token punctuation\">.</span>stdout<span class=\"token punctuation\">.</span>write<span class=\"token punctuation\">(</span><span class=\"token string\">'\\r前処理中 {} / {}'</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>idx<span class=\"token punctuation\">,</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>corpus<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">yield</span> split_into_words<span class=\"token punctuation\">(</span>doc<span class=\"token punctuation\">,</span> name<span class=\"token punctuation\">)</span></code></pre></div>\n<p>ファイルから文章を取得し、単語に分解します。<br>\n精度を上げるために、学習に使う単語を名詞のみにすることがあるようです。<br>\n今回は動詞・形容詞・名詞(数詞以外)にしました。</p>\n<h4 id=\"5-doc2vecで学習\" style=\"position:relative;\"><a href=\"#5-doc2vec%E3%81%A7%E5%AD%A6%E7%BF%92\" aria-label=\"5 doc2vecで学習 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>5. Doc2Vecで学習</h4>\n<div class=\"gatsby-highlight\" data-language=\"py\"><pre class=\"language-py\"><code class=\"language-py\"><span class=\"token keyword\">def</span> <span class=\"token function\">train</span><span class=\"token punctuation\">(</span>sentences<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    model <span class=\"token operator\">=</span> models<span class=\"token punctuation\">.</span>Doc2Vec<span class=\"token punctuation\">(</span>size<span class=\"token operator\">=</span><span class=\"token number\">400</span><span class=\"token punctuation\">,</span> alpha<span class=\"token operator\">=</span><span class=\"token number\">0.0015</span><span class=\"token punctuation\">,</span> sample<span class=\"token operator\">=</span><span class=\"token number\">1e</span><span class=\"token operator\">-</span><span class=\"token number\">4</span><span class=\"token punctuation\">,</span> min_count<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> workers<span class=\"token operator\">=</span><span class=\"token number\">4</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>build_vocab<span class=\"token punctuation\">(</span>sentences<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token number\">30</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n        model<span class=\"token punctuation\">.</span>train<span class=\"token punctuation\">(</span>sentences<span class=\"token punctuation\">)</span>\n        ranks <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n        <span class=\"token keyword\">for</span> doc_id <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token number\">100</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            inferred_vector <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>infer_vector<span class=\"token punctuation\">(</span>sentences<span class=\"token punctuation\">[</span>doc_id<span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>words<span class=\"token punctuation\">)</span>\n            sims <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>docvecs<span class=\"token punctuation\">.</span>most_similar<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>inferred_vector<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> topn<span class=\"token operator\">=</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">.</span>docvecs<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n            rank <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>docid <span class=\"token keyword\">for</span> docid<span class=\"token punctuation\">,</span> sim <span class=\"token keyword\">in</span> sims<span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>index<span class=\"token punctuation\">(</span>sentences<span class=\"token punctuation\">[</span>doc_id<span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>tags<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n            ranks<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>rank<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>collections<span class=\"token punctuation\">.</span>Counter<span class=\"token punctuation\">(</span>ranks<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">if</span> collections<span class=\"token punctuation\">.</span>Counter<span class=\"token punctuation\">(</span>ranks<span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">>=</span> PASSING_PRECISION<span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">break</span>\n    <span class=\"token keyword\">return</span> model</code></pre></div>\n<p>models.Doc2Vecの部分で学習時のパラメータを設定しています。</p>\n<ul>\n<li>size: ベクトル化した際の次元数</li>\n<li>alpha: 学習率</li>\n<li>sample: 単語を無視する際の頻度の閾値</li>\n<li>min_count: 学習に使う単語の最低出現回数</li>\n<li>workers: 学習時のスレッド数</li>\n</ul>\n<h5 id=\"alpha\" style=\"position:relative;\"><a href=\"#alpha\" aria-label=\"alpha permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>alpha</h5>\n<p>高いほど収束が速いですが、高すぎると発散します。<br>\n低いほど精度が高いですが、収束が遅くなります。</p>\n<h5 id=\"sample\" style=\"position:relative;\"><a href=\"#sample\" aria-label=\"sample permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>sample</h5>\n<p>あまりに高い頻度で出現する単語は意味のない単語である可能性が高いので、無視することがあります。<br>\nその閾値を設定します。</p>\n<h5 id=\"min_count\" style=\"position:relative;\"><a href=\"#min_count\" aria-label=\"min_count permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>min_count</h5>\n<p>sampleとは逆に、頻度が少なすぎる単語もその文章を表現するのに適切でない場合があるので無視することがあります。<br>\nただ、今回はすべての単語を対象にしました。</p>\n<div class=\"gatsby-highlight\" data-language=\"py\"><pre class=\"language-py\"><code class=\"language-py\"><span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token number\">30</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\n        model<span class=\"token punctuation\">.</span>train<span class=\"token punctuation\">(</span>sentences<span class=\"token punctuation\">)</span>\n        ranks <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n        <span class=\"token keyword\">for</span> doc_id <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token number\">100</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            inferred_vector <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>infer_vector<span class=\"token punctuation\">(</span>sentences<span class=\"token punctuation\">[</span>doc_id<span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>words<span class=\"token punctuation\">)</span>\n            sims <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>docvecs<span class=\"token punctuation\">.</span>most_similar<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>inferred_vector<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> topn<span class=\"token operator\">=</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">.</span>docvecs<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n            rank <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>docid <span class=\"token keyword\">for</span> docid<span class=\"token punctuation\">,</span> sim <span class=\"token keyword\">in</span> sims<span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>index<span class=\"token punctuation\">(</span>sentences<span class=\"token punctuation\">[</span>doc_id<span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>tags<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n            ranks<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>rank<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>collections<span class=\"token punctuation\">.</span>Counter<span class=\"token punctuation\">(</span>ranks<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">if</span> collections<span class=\"token punctuation\">.</span>Counter<span class=\"token punctuation\">(</span>ranks<span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">>=</span> PASSING_PRECISION<span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">break</span>\n    <span class=\"token keyword\">return</span> model</code></pre></div>\n<p>この部分で学習と評価を行っています。<br>\n評価は、学習した文章のうち100個で類似の文章を検索し、最も類似度の高い文章が自分自身だった回数で行います。<br>\n今回は94回以上の場合に学習を終了するようにしました。<br>\n(何回か回してみてそれ以上に精度が上がらなかったため)</p>\n<h4 id=\"6-学習データを出力\" style=\"position:relative;\"><a href=\"#6-%E5%AD%A6%E7%BF%92%E3%83%87%E3%83%BC%E3%82%BF%E3%82%92%E5%87%BA%E5%8A%9B\" aria-label=\"6 学習データを出力 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>6. 学習データを出力</h4>\n<div class=\"gatsby-highlight\" data-language=\"py\"><pre class=\"language-py\"><code class=\"language-py\">model<span class=\"token punctuation\">.</span>save<span class=\"token punctuation\">(</span>OUTPUT_MODEL<span class=\"token punctuation\">)</span></code></pre></div>\n<p>OUTPUT_MODELには出力するパスが入ります。</p>\n<h3 id=\"単語で文章を検索\" style=\"position:relative;\"><a href=\"#%E5%8D%98%E8%AA%9E%E3%81%A7%E6%96%87%E7%AB%A0%E3%82%92%E6%A4%9C%E7%B4%A2\" aria-label=\"単語で文章を検索 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>単語で文章を検索</h3>\n<div class=\"gatsby-highlight\" data-language=\"py\"><pre class=\"language-py\"><code class=\"language-py\">model <span class=\"token operator\">=</span> models<span class=\"token punctuation\">.</span>Doc2Vec<span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span><span class=\"token string\">'doc2vec.model'</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">search_similar_texts</span><span class=\"token punctuation\">(</span>words<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    x <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>infer_vector<span class=\"token punctuation\">(</span>words<span class=\"token punctuation\">)</span>\n    most_similar_texts <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>docvecs<span class=\"token punctuation\">.</span>most_similar<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>x<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">for</span> similar_text <span class=\"token keyword\">in</span> most_similar_texts<span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>similar_text<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>Doc2Vecでは同時に単語のベクトル化(Word2Vec)も行うので、類似単語も検索するようにしました。</p>\n<div class=\"gatsby-highlight\" data-language=\"py\"><pre class=\"language-py\"><code class=\"language-py\"><span class=\"token keyword\">def</span> <span class=\"token function\">search_similar_words</span><span class=\"token punctuation\">(</span>words<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">for</span> word <span class=\"token keyword\">in</span> words<span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>word <span class=\"token operator\">+</span> <span class=\"token string\">':'</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">for</span> result <span class=\"token keyword\">in</span> model<span class=\"token punctuation\">.</span>most_similar<span class=\"token punctuation\">(</span>positive<span class=\"token operator\">=</span>word<span class=\"token punctuation\">,</span> topn<span class=\"token operator\">=</span><span class=\"token number\">10</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>result<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h4 id=\"「猫」で検索した例\" style=\"position:relative;\"><a href=\"#%E3%80%8C%E7%8C%AB%E3%80%8D%E3%81%A7%E6%A4%9C%E7%B4%A2%E3%81%97%E3%81%9F%E4%BE%8B\" aria-label=\"「猫」で検索した例 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>「猫」で検索した例</h4>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/bb96074b5a5f5c344948b3212b343c8e/20751/cat.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 62.0253164556962%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAA7DAAAOwwHHb6hkAAAA8ElEQVQoz62R3UrDQBCFZze72PwQiGZrqyZNre2GxZtctMlVIEHFt/D9n+KYDkUqFCVtLw4Dc/GdM3No/viEqTFI0zsYkyIIAijPAxGN1iLPQdliibqu0XUd+r5HVVWw1iJJEoZrrSGlhBCS5yl5hwDOuQFYLLHbbtG2LQObpkE+OPm+z1JK/ZtMCMGzLEvQdPaAoijwsl7zIsuy4XTD6caezMDb1PzpOhq42tizASeBm9JdDPoFNPczhGGIOI75b1EU/Wi/37c4CkhCXiHdUcsk1XVPJk8fuZypQ6HWDsDJ6ydu3PtFmrgPaNvj+e0L39EWC6QtIusWAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"cat\"\n        title=\"cat\"\n        src=\"/static/bb96074b5a5f5c344948b3212b343c8e/f058b/cat.png\"\n        srcset=\"/static/bb96074b5a5f5c344948b3212b343c8e/c26ae/cat.png 158w,\n/static/bb96074b5a5f5c344948b3212b343c8e/6bdcf/cat.png 315w,\n/static/bb96074b5a5f5c344948b3212b343c8e/f058b/cat.png 630w,\n/static/bb96074b5a5f5c344948b3212b343c8e/40601/cat.png 945w,\n/static/bb96074b5a5f5c344948b3212b343c8e/20751/cat.png 1037w\"\n        sizes=\"(max-width: 630px) 100vw, 630px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h4 id=\"「雪」で検索した例\" style=\"position:relative;\"><a href=\"#%E3%80%8C%E9%9B%AA%E3%80%8D%E3%81%A7%E6%A4%9C%E7%B4%A2%E3%81%97%E3%81%9F%E4%BE%8B\" aria-label=\"「雪」で検索した例 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>「雪」で検索した例</h4>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/ae031528ff2b8dd135d4ac2548d1f3fe/20751/snow.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 62.0253164556962%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAA7DAAAOwwHHb6hkAAAA7klEQVQoz62S3W6CMBiGv9Li5G8hIUpUoGwQaZqdCgkkuMi27C52/1fxTruZeGImjoMn7dHzfn+02iRYLhaIoghxHCMMQ8xmNohoNDLLQKl8QtM02O9fMQwD2raFlBJBEMBxHHAuwBgDsyxYV+CcG6HW+kdY73bo+x5d16GuayMUQtxc2Snw9CqlQMvVGnmeoyxLFEWBNE1N++fUMRhhIvO75nVVuFUa9oj2/hSWW2UGO5lwnWTwff+41Ue4rmv+ZzzPGxVmhERsguoutswsMe1SiNsXKXfye4dVdRTOXz7xoN//xVx/wK4OeH77wjcbWQty2rt3+gAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"snow\"\n        title=\"snow\"\n        src=\"/static/ae031528ff2b8dd135d4ac2548d1f3fe/f058b/snow.png\"\n        srcset=\"/static/ae031528ff2b8dd135d4ac2548d1f3fe/c26ae/snow.png 158w,\n/static/ae031528ff2b8dd135d4ac2548d1f3fe/6bdcf/snow.png 315w,\n/static/ae031528ff2b8dd135d4ac2548d1f3fe/f058b/snow.png 630w,\n/static/ae031528ff2b8dd135d4ac2548d1f3fe/40601/snow.png 945w,\n/static/ae031528ff2b8dd135d4ac2548d1f3fe/20751/snow.png 1037w\"\n        sizes=\"(max-width: 630px) 100vw, 630px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h3 id=\"類似文章の検索\" style=\"position:relative;\"><a href=\"#%E9%A1%9E%E4%BC%BC%E6%96%87%E7%AB%A0%E3%81%AE%E6%A4%9C%E7%B4%A2\" aria-label=\"類似文章の検索 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>類似文章の検索</h3>\n<div class=\"gatsby-highlight\" data-language=\"py\"><pre class=\"language-py\"><code class=\"language-py\">model <span class=\"token operator\">=</span> models<span class=\"token punctuation\">.</span>Doc2Vec<span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span><span class=\"token string\">'doc2vec.model'</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">def</span> <span class=\"token function\">search_similar_texts</span><span class=\"token punctuation\">(</span>path<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    most_similar_texts <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>docvecs<span class=\"token punctuation\">.</span>most_similar<span class=\"token punctuation\">(</span>path<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">for</span> similar_text <span class=\"token keyword\">in</span> most_similar_texts<span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>similar_text<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h4 id=\"夏目漱石の「吾輩は猫である」で検索した例\" style=\"position:relative;\"><a href=\"#%E5%A4%8F%E7%9B%AE%E6%BC%B1%E7%9F%B3%E3%81%AE%E3%80%8C%E5%90%BE%E8%BC%A9%E3%81%AF%E7%8C%AB%E3%81%A7%E3%81%82%E3%82%8B%E3%80%8D%E3%81%A7%E6%A4%9C%E7%B4%A2%E3%81%97%E3%81%9F%E4%BE%8B\" aria-label=\"夏目漱石の「吾輩は猫である」で検索した例 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>夏目漱石の「吾輩は猫である」で検索した例</h4>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/618eb1949972b1e96faa167d9c4b13ee/20751/natsume.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 62.0253164556962%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAA7DAAAOwwHHb6hkAAAA5ElEQVQoz62RyYqDQBRFa3AqY0Q6uEh3a8QIStkgxKzUIEh3k7/I/3/FjWVCSCCLDLU41FsU593LI0m6BiFEC6s4BvmOE3wul/hYLBAEAXzfh+d54JyDUgrGGKiCsmm+h/qrhFJKkK8oRl1v0HU7DMMwzjWyLINhGA8nU4vVWxTFKeF2W6NtW/R9PwmrqkIYhk9XnoRJmp0TdmiaBmVZTgmjKILrus8LxWwO4TgQQsCyrBts277UeVhICNN25ZOQG5qFTIeQXgu57srm1ZYXOR8uz0eh87OHLf/ewpH/MPMB6e8BRxhxC6kbhW5NAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"natsume\"\n        title=\"natsume\"\n        src=\"/static/618eb1949972b1e96faa167d9c4b13ee/f058b/natsume.png\"\n        srcset=\"/static/618eb1949972b1e96faa167d9c4b13ee/c26ae/natsume.png 158w,\n/static/618eb1949972b1e96faa167d9c4b13ee/6bdcf/natsume.png 315w,\n/static/618eb1949972b1e96faa167d9c4b13ee/f058b/natsume.png 630w,\n/static/618eb1949972b1e96faa167d9c4b13ee/40601/natsume.png 945w,\n/static/618eb1949972b1e96faa167d9c4b13ee/20751/natsume.png 1037w\"\n        sizes=\"(max-width: 630px) 100vw, 630px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h4 id=\"太宰治の「人間失格」で検索した例\" style=\"position:relative;\"><a href=\"#%E5%A4%AA%E5%AE%B0%E6%B2%BB%E3%81%AE%E3%80%8C%E4%BA%BA%E9%96%93%E5%A4%B1%E6%A0%BC%E3%80%8D%E3%81%A7%E6%A4%9C%E7%B4%A2%E3%81%97%E3%81%9F%E4%BE%8B\" aria-label=\"太宰治の「人間失格」で検索した例 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>太宰治の「人間失格」で検索した例</h4>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 630px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/a697fdf83ecc1bb8f213163ed0afec36/20751/dazai.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 62.0253164556962%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAAA7DAAAOwwHHb6hkAAAA8UlEQVQoz62SzU6EMBRG++NAywAhsEJngExMIAUWJFhla3Qyb+H7P8U3bXUmE11gtIuT3kV7+t3bkvvdHoQQL9RVBVIaYZHnSJIEaZo6wjBEEATgnIMx5qCUXevv2H1WqJQCedjXeJpnLMsCrTXGcURZloii6NfJKKVu7boOZFfVmI3wxQitdJomVCZ6URTIssylvRxYwwmbw6MTav2MYRjQ9z2apnEpczOKOI6vo7DtrQrlNoEUAlJKl0aY+jJDy5rkh5AQ5u2VP4X8zrOQ+RDSWyH33fLm5pY/8vWt2tYIxXBCqN7/hVBHbNpXHN4+cAbOLQteyAjkVgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"dazai\"\n        title=\"dazai\"\n        src=\"/static/a697fdf83ecc1bb8f213163ed0afec36/f058b/dazai.png\"\n        srcset=\"/static/a697fdf83ecc1bb8f213163ed0afec36/c26ae/dazai.png 158w,\n/static/a697fdf83ecc1bb8f213163ed0afec36/6bdcf/dazai.png 315w,\n/static/a697fdf83ecc1bb8f213163ed0afec36/f058b/dazai.png 630w,\n/static/a697fdf83ecc1bb8f213163ed0afec36/40601/dazai.png 945w,\n/static/a697fdf83ecc1bb8f213163ed0afec36/20751/dazai.png 1037w\"\n        sizes=\"(max-width: 630px) 100vw, 630px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h2 id=\"まとめ\" style=\"position:relative;\"><a href=\"#%E3%81%BE%E3%81%A8%E3%82%81\" aria-label=\"まとめ permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>まとめ</h2>\n<p>Doc2Vecで類似文章の検索を実装してみました。<br>\n少しでも参考になれば幸いです。</p>\n<h2 id=\"エラーが起きた場合\" style=\"position:relative;\"><a href=\"#%E3%82%A8%E3%83%A9%E3%83%BC%E3%81%8C%E8%B5%B7%E3%81%8D%E3%81%9F%E5%A0%B4%E5%90%88\" aria-label=\"エラーが起きた場合 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>エラーが起きた場合</h2>\n<p>私の環境で起きたエラーのみですが、解決した方法を載せます。</p>\n<ul>\n<li><code class=\"language-text\">Intel MKL FATAL ERROR: Cannot load libmkl_avx2.so or libmkl_def.so.</code><br>\nanaconda3-4.2.0を使っている場合に起きることがあるようです。<br>\n<code class=\"language-text\">conda update numpy</code>で解決しました。</li>\n<li>Invalid argumentのエラー<br>\nBash on Ubuntu on Windowsを使っている場合に起きることがあるようです。<br>\n<code class=\"language-text\">export KMP_AFFINITY=disabled</code>で解決しました。</li>\n</ul>\n<h2 id=\"参考\" style=\"position:relative;\"><a href=\"#%E5%8F%82%E8%80%83\" aria-label=\"参考 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>参考</h2>\n<p><a href=\"https://deepage.net/bigdata/machine_learning/2016/09/02/word2vec_power_of_word_vector.html\">Word2Vec：発明した本人も驚く単語ベクトルの驚異的な力</a><br>\n<a href=\"https://deepage.net/machine_learning/2017/01/08/doc2vec.html\">Doc2Vecの仕組みとgensimを使った文書類似度算出チュートリアル</a><br>\n<a href=\"http://inside.pixiv.net/entry/2016/09/13/161454\">pixiv小説で機械学習したらどうなるのっと【学習済みモデルデータ配布あり】</a><br>\n<a href=\"http://qiita.com/isaac-otao/items/6d44fdc0cfc8fed53657\">TensorFlowを使って学習率による動きの違いを確認する</a><br>\n<a href=\"https://radimrehurek.com/gensim/models/doc2vec.html\">models.doc2vec – Deep learning with paragraph2vec</a></p>\n<!-- リンク -->","frontmatter":{"title":"文章をベクトル化して類似文章の検索","date":"2017-02-26","description":null,"header":{"childImageSharp":{"fluid":{"aspectRatio":1.5,"src":"/static/f27fb428c244155a8bdbee7cf7f68de2/d8255/header.jpg","srcSet":"/static/f27fb428c244155a8bdbee7cf7f68de2/9104c/header.jpg 480w,\n/static/f27fb428c244155a8bdbee7cf7f68de2/a6352/header.jpg 960w,\n/static/f27fb428c244155a8bdbee7cf7f68de2/d8255/header.jpg 1920w","sizes":"(max-width: 1920px) 100vw, 1920px"}}},"ogp":{"publicURL":"/static/f5715df17178255085d9663920fff296/ogp.jpg"},"tags":["python","word2vec","doc2vec"]},"tableOfContents":"<ul>\n<li><a href=\"/search-with-doc2vec/#doc2vec%E3%81%A8%E3%81%AF\">Doc2Vecとは</a></li>\n<li>\n<p><a href=\"/search-with-doc2vec/#%E5%AE%9F%E8%A3%85%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB\">実装サンプル</a></p>\n<ul>\n<li><a href=\"/search-with-doc2vec/#%E7%92%B0%E5%A2%83\">環境</a></li>\n<li><a href=\"/search-with-doc2vec/#%E5%AD%A6%E7%BF%92\">学習</a></li>\n<li><a href=\"/search-with-doc2vec/#%E5%8D%98%E8%AA%9E%E3%81%A7%E6%96%87%E7%AB%A0%E3%82%92%E6%A4%9C%E7%B4%A2\">単語で文章を検索</a></li>\n<li><a href=\"/search-with-doc2vec/#%E9%A1%9E%E4%BC%BC%E6%96%87%E7%AB%A0%E3%81%AE%E6%A4%9C%E7%B4%A2\">類似文章の検索</a></li>\n</ul>\n</li>\n<li><a href=\"/search-with-doc2vec/#%E3%81%BE%E3%81%A8%E3%82%81\">まとめ</a></li>\n<li><a href=\"/search-with-doc2vec/#%E3%82%A8%E3%83%A9%E3%83%BC%E3%81%8C%E8%B5%B7%E3%81%8D%E3%81%9F%E5%A0%B4%E5%90%88\">エラーが起きた場合</a></li>\n<li><a href=\"/search-with-doc2vec/#%E5%8F%82%E8%80%83\">参考</a></li>\n</ul>"}},"pageContext":{"id":"6c5b5aed-ddee-5c03-8d1d-f2703bbdd601"}},"staticQueryHashes":["3159585216","3357329864","3862782001","4070466330"]}